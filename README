


1) Create or update conda env:
conda env create -f environment.yml -n <proj_name>
conda env update -f environment.yml -n <proj_name>

activate <proj_name>

2) If needed, edit and run scripts/download_dataset.sh
3) Search all TODO and implement accordingly
4) 


When env is stable, generate the requirements file for reproducing the analysis environment "requirements.txt"
by running `pip freeze > requirements.txt`


Env structure according to cookiecutter-data-science, with slight additions -

├── LICENSE
├── Makefile                    <- Makefile with commands like `make data` or `make train`
├── README.md                   <- The top-level README for developers using this project.
├── data
│   ├── external                <- Data from third party sources.
│   ├── interim                 <- Intermediate data that has been transformed.
│   ├── processed               <- The final, canonical data sets for modeling.
│   └── raw                     <- The original, immutable data dump.
│
├── models                      <- Trained and serialized models, model predictions, or model summaries
│
├── scripts                     <- Various scripts - download dataset, generate synthetic data, etc...
│   ├── download.py             <- download data from an external source
│   ├── make_dataset.py         <- Create a dataset
│
├── notebooks                   <- Jupyter notebooks. Naming convention is a number (for ordering),
│                                  the creator's initials, and a short `-` delimited description, e.g.
│                                  `1.0-jqp-initial-data-exploration`.
│
├── reports                     <- Generated analysis as HTML, PDF, LaTeX, etc.
│   └── figures                 <- Generated graphics and figures to be used in reporting
│
├── requirements.txt            <- The requirements file for reproducing the analysis environment, e.g.
│                                  generated with `pip freeze > requirements.txt`
│
├── src                         <- Source code for use in this project.
│   ├── __init__.py             <- Makes src a Python module
│   │
│   ├── datasets                <- Custom PyTorch data loaders for each dataset
│   ├── models                  <- Scripts to train models and then use trained models to make predictions
│   │   └── tools               <- holds all files with functions
│   │   │   └── eval_tools.py   <- 
│   │   │   └── logger_tools.py <-
│   │   │   └── model_tools.py  <-
│   │   │   └── rec_tools.py    <- functions for keeping track of training history
│   │   │   └── test_tools.py   <-
│   │   │   └── train_tools.py  <-
│   └── main_flow.py        <- will run a train and test flow, according to arguments
│   └── options.py          <- various arguments to control the test, train, models, etc...
│   │
│   └── visualization           <- Scripts to create exploratory and results oriented visualizations
│       └── visualize.py